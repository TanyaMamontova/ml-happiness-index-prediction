# -*- coding: utf-8 -*-
"""проект прогнозирование индекса счастья

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17pEIL2qGUqu9Y9kbR56Y5GQAIiWzEJPX
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline

# Импорт данных
df = pd.read_csv(r'/content/2018 (2).csv')

# Смотрим общую информацию по данным и первые строки
print("Информация о датафрейме:")
df.info()

print("Первые строки датафрейма:")
df.head()

# Удаляем колонку Overall rank, так как это просто индекс рейтинга,
# который напрямую зависит от Score и будет мешать моделям
df.drop(["Overall rank"], axis=1, inplace = True)

#проверка пропущенных значений
def data_gapes(data):
    gapes = data.isnull().sum()
    return gapes
print(data_gapes(df),  "\n")

#выявление дубликатов
def data_dublicates(data):
    dublicats = data[data.duplicated()]
    return dublicats
print (data_dublicates(df),  "\n")

#Считаем базовые статистики по всем числовым признакам
print("Описательная статистика:\n")
df.describe()

# Анализ распределения данных (гистограммы + boxplot)
numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
# исключим страну
if "Country or region" in numeric_columns:
    numeric_columns.remove("Country or region")

for col in numeric_columns:
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    df[col].hist(bins=30)
    plt.title(f'Гистограмма {col}')
    plt.subplot(1, 2, 2)
    df[col].plot(kind='box')
    plt.title(f'Boxplot {col}')
    plt.tight_layout()
    plt.show()

#ДОПОЛНИТЕЛЬНЫЕ ГРАФИКИ ПО SCORE
# 1) Распределение Score
df['Score'].plot(kind='hist', bins=30, color='pink', edgecolor='black')
plt.xlabel('Индекс счастья')
plt.ylabel('Количество стран')
plt.title('Распределение индекса счастья')
plt.show()

# 2) Топ-30 самых счастливых стран
region = df.groupby("Country or region", as_index=False)["Score"].mean()
region = region.sort_values("Score").reset_index(drop=True)
top30 = region.tail(30)

fig, ax = plt.subplots(figsize=(10, 20))
ax.barh(range(len(top30.index)), top30["Score"], align='edge', height=.5, color='MediumAquamarine')
ax.set_yticks(range(len(top30.index)))
ax.set_yticklabels(top30["Country or region"])
plt.ylim(0, len(top30.index))
plt.xlabel('Индекс счастья')
plt.ylabel('Страна или регион')
plt.title('Топ-30 самых счастливых стран')
plt.show()

# 3) Топ-30 самых несчастливых стран
bottom30 = region.head(30)
fig, ax = plt.subplots(figsize=(10, 20))
ax.barh(range(len(bottom30.index)), bottom30["Score"], align='edge', height=.5, color='Peru')
ax.set_yticks(range(len(bottom30.index)))
ax.set_yticklabels(bottom30["Country or region"])
plt.ylim(0, len(bottom30.index))
plt.xlabel('Индекс счастья')
plt.ylabel('Страна или регион')
plt.title('Топ-30 самых несчастливых стран')
plt.show()

outlier_cols = ['GDP per capita', 'Social support', 'Healthy life expectancy',
                'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']

# Проверка на наличие выбросов по методу IQR
def iqr_outliers_report(data, cols, k=1.5):
    for col in cols:
        q1 = data[col].quantile(0.25)
        q3 = data[col].quantile(0.75)
        iqr = q3 - q1
        lower = q1 - k * iqr
        upper = q3 + k * iqr
        outliers = data[(data[col] < lower) | (data[col] > upper)]
        print(f"{col}: {len(outliers)} выбросов из {len(data)} строк")

print("Выбросы в данных:")
iqr_outliers_report(df, outlier_cols)

#Винсоризация выбрсов
def winsorize_series(s, k=1.5):
    q1, q3 = s.quantile([0.25, 0.75])
    iqr = q3 - q1
    lower, upper = q1 - k*iqr, q3 + k*iqr
    return s.clip(lower, upper)

for col in outlier_cols :
    if col in df.columns:
        df[col] = winsorize_series(df[col])

print("Выбросы после винзоризации:")
iqr_outliers_report(df, outlier_cols)

#Корреляционная матрица
new_df1 = df[['Score', 'GDP per capita', 'Social support', 'Healthy life expectancy',
              'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']].copy()
plt.figure(figsize=(10, 8))
sns.heatmap(new_df1.corr(), annot=True, cbar=False)
plt.title("Корреляционная матрица")
plt.show()

# Построение и обучение моделей машинного обучения

#Признаки и целевая переменная
X_all = df.drop(columns=['Country or region', 'Score'])
y = df['Score']
x_train, x_test, y_train, y_test = train_test_split(X_all, y, random_state=2, test_size=0.2)

# Метрики
def check_metrics(y_pred, y_test, name):
    print('--', name, '--')
    print('R²:', r2_score(y_test, y_pred))
    print('MAPE:', mean_absolute_percentage_error(y_test, y_pred))
    print('MAE:', mean_absolute_error(y_test, y_pred))
    print('RMSE:', np.sqrt(mean_squared_error(y_test, y_pred)))

#Функции моделей
def model_linreg(x_train, x_test, y_train, y_test):
    scaler = StandardScaler()
    x_train_scaled = scaler.fit_transform(x_train)
    x_test_scaled = scaler.transform(x_test)

    reg = LinearRegression()
    reg.fit(x_train_scaled, y_train)
    y_pred = reg.predict(x_test_scaled)
    check_metrics(y_pred, y_test, "Линейная регрессия")

def model_ridge(x_train, x_test, y_train, y_test, alpha=1.0):
    scaler = StandardScaler()
    x_train_scaled = scaler.fit_transform(x_train)
    x_test_scaled = scaler.transform(x_test)

    reg = Ridge(alpha=alpha)
    reg.fit(x_train_scaled, y_train)
    y_pred = reg.predict(x_test_scaled)
    check_metrics(y_pred, y_test, "Ridge регрессия")

def model_poly(x_all, y, degree=2):
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly.fit_transform(x_all)
    x_train_p, x_test_p, y_train_p, y_test_p = train_test_split(X_poly, y, random_state=2, test_size=0.2)

    scaler = StandardScaler()
    x_train_p_scaled = scaler.fit_transform(x_train_p)
    x_test_p_scaled = scaler.transform(x_test_p)

    reg = LinearRegression()
    reg.fit(x_train_p_scaled, y_train_p)
    y_pred = reg.predict(x_test_p_scaled)
    check_metrics(y_pred, y_test_p, f"Полиномиальная регрессия (deg={degree})")

def model_rf(x_train, x_test, y_train, y_test):
    print("\nОбучаем Random Forest с подбором гиперпараметров...")
    rf = RandomForestRegressor(random_state=2)   # сначала создаём модель
    param_grid_rf = {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 5, 10],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2]
    }
    grid = GridSearchCV(rf, param_grid_rf, cv=5, scoring='r2', n_jobs=-1)
    grid.fit(x_train, y_train)
    print("Лучшие параметры RandomForest:", grid.best_params_)
    y_pred = grid.predict(x_test)
    check_metrics(y_pred, y_test, "Случайный лес")

def model_knn(x_train, x_test, y_train, y_test):
    print("\nОбучаем KNN с подбором гиперпараметров...")
    knn = KNeighborsRegressor()
    pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("knn", knn)
    ])
    param_grid_knn = {
        'knn__n_neighbors': [3, 5, 7, 9, 11],
        'knn__weights': ['uniform', 'distance']
    }
    grid = GridSearchCV(pipe, param_grid_knn, cv=5, scoring='r2', n_jobs=-1)
    grid.fit(x_train, y_train)
    print("Лучшие параметры KNN:", grid.best_params_)
    y_pred = grid.predict(x_test)
    check_metrics(y_pred, y_test, "KNN (GridSearchCV)")

model_linreg(x_train, x_test, y_train, y_test)
model_ridge(x_train, x_test, y_train, y_test)
model_poly(X_all, y, degree=2)
model_rf(x_train, x_test, y_train, y_test)
model_knn(x_train, x_test, y_train, y_test)